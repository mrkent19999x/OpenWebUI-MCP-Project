# Nginx Configuration - Load Balancer for Multi-Agent Performance
# Optimized for MiniMax-like speed and responsiveness

user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 2048;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;

    # Performance Optimizations
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    keepalive_requests 1000;
    types_hash_max_size 2048;
    server_tokens off;

    # Gzip Compression
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types text/plain text/css text/xml text/javascript 
               application/json application/javascript application/xml+rss 
               application/rss+xml font/truetype font/opentype 
               application/vnd.ms-fontobject image/svg+xml;

    # Buffer Sizes (Optimized for Large Responses)
    client_body_buffer_size 128k;
    client_max_body_size 100m;
    client_header_buffer_size 1k;
    large_client_header_buffers 4 16k;
    output_buffers 1 32k;
    postpone_output 1460;

    # Timeouts (Optimized for Long-running Requests)
    client_body_timeout 300;
    client_header_timeout 300;
    send_timeout 300;
    proxy_connect_timeout 300;
    proxy_send_timeout 300;
    proxy_read_timeout 300;

    # Upstream - OpenWebUI Backend
    upstream openwebui_backend {
        # Single instance
        server open-webui:8080 max_fails=3 fail_timeout=30s;
        
        # Keep connections alive
        keepalive 32;
    }

    # LiteLLM Gateway Backend - Multi-Provider AI Models
    upstream litellm_backend {
        # Load balancing method: least_conn for optimal performance
        least_conn;
        
        # LiteLLM Gateway
        server litellm:4000 max_fails=3 fail_timeout=30s;
        
        # Keep connections alive for streaming
        keepalive 64;
    }

    # Rate Limiting (Prevent Abuse - Enhanced)
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=100r/s;
    limit_req_zone $binary_remote_addr zone=chat_limit:10m rate=50r/s;
    limit_req_zone $binary_remote_addr zone=litellm_limit:10m rate=200r/s;

    # Cache Configuration (Smart Caching like MiniMax)
    proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=webui_cache:10m 
                     max_size=1g inactive=60m use_temp_path=off;
    
    # LiteLLM Cache Configuration
    proxy_cache_path /var/cache/nginx/litellm levels=1:2 keys_zone=litellm_cache:20m 
                     max_size=2g inactive=120m use_temp_path=off;

    # Main Server Block
    server {
        listen 80;
        server_name _;

        # Security Headers
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-XSS-Protection "1; mode=block" always;

        # WebSocket Support (for Real-time Chat)
        location / {
            proxy_pass http://openwebui_backend;
            
            # WebSocket Headers
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            
            # Standard Proxy Headers
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Buffering (Disabled for Streaming)
            proxy_buffering off;
            proxy_cache off;
            
            # Timeouts
            proxy_connect_timeout 300s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
            
            # Rate Limiting
            limit_req zone=chat_limit burst=20 nodelay;
        }

        # API Endpoints (Cached)
        location /api/ {
            proxy_pass http://openwebui_backend;
            
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Enable Caching
            proxy_cache webui_cache;
            proxy_cache_valid 200 302 10m;
            proxy_cache_valid 404 1m;
            proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
            proxy_cache_background_update on;
            proxy_cache_lock on;
            
            add_header X-Cache-Status $upstream_cache_status;
            
            # Rate Limiting
            limit_req zone=api_limit burst=50 nodelay;
        }

        # Static Files (Cached Aggressively)
        location ~* \.(jpg|jpeg|png|gif|ico|css|js|svg|woff|woff2|ttf|eot)$ {
            proxy_pass http://openwebui_backend;
            expires 1y;
            add_header Cache-Control "public, immutable";
            access_log off;
        }

        # Health Check Endpoint
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }

        # LiteLLM Gateway API Routes (100+ AI Models)
        location /v1/ {
            proxy_pass http://litellm_backend;
            
            # WebSocket support for streaming
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            
            # Standard headers
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Enable caching for responses
            proxy_cache litellm_cache;
            proxy_cache_valid 200 302 5m;
            proxy_cache_valid 404 1m;
            proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
            proxy_cache_background_update on;
            proxy_cache_lock on;
            
            # Timeouts for AI model processing
            proxy_connect_timeout 300s;
            proxy_send_timeout 600s;
            proxy_read_timeout 600s;
            
            # Rate limiting
            limit_req zone=litellm_limit burst=100 nodelay;
            
            add_header X-Cache-Status $upstream_cache_status;
        }

        # LiteLLM Health Check
        location /litellm-health {
            access_log off;
            proxy_pass http://litellm_backend/health;
            proxy_set_header Host $host;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            return 200 "litellm healthy\n";
            add_header Content-Type text/plain;
        }
    }
}
