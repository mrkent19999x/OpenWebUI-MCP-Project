# ğŸš€ Open WebUI Stack - HÆ°á»›ng Dáº«n CÃ i Äáº·t Chi Tiáº¿t 2025

## ğŸ“‹ Tá»•ng Quan
Há»‡ thá»‘ng AI Chat Interface Ä‘a ná»n táº£ng vá»›i **5 giao diá»‡n chÃ­nh** vÃ  **10+ nhÃ  cung cáº¥p API miá»…n phÃ­**, tÃ­ch há»£p LiteLLM Ä‘á»ƒ tá»± Ä‘á»™ng routing vÃ  failover.

### ğŸ—ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NGINX REVERSE PROXY                  â”‚
â”‚                  (Load Balancer + SSL)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         â”‚         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ OPEN WEBUI  â”‚ â”‚LOBECHATâ”‚ â”‚ANYTHING â”‚ â”‚LIBRECHATâ”‚ â”‚  BIGAGI â”‚
â”‚ Port: 3000  â”‚ â”‚ 3001  â”‚ â”‚  3002   â”‚ â”‚  3003  â”‚ â”‚  3004  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚         â”‚         â”‚         â”‚         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚         â”‚         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LITELLM API GATEWAY              â”‚
â”‚                Port: 4000                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚         â”‚         â”‚         â”‚         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
â”‚   CEREBRAS  â”‚ â”‚CF AI  â”‚ â”‚GITHUB â”‚ â”‚OPENRT â”‚ â”‚GEMINI â”‚
â”‚ 1M tokens/ â”‚ â”‚10K    â”‚ â”‚Multi  â”‚ â”‚Varietyâ”‚ â”‚Ultra  â”‚
â”‚ day         â”‚ â”‚neuron â”‚ â”‚modal  â”‚ â”‚       â”‚ â”‚tokens â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               OLLAMA LOCAL                   â”‚
â”‚          (Fallback when offline)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”¥ TÃ­nh NÄƒng Ná»•i Báº­t

### ğŸ¯ **Multi-Platform UI**
- **Open WebUI** (58k â­) - Giao diá»‡n gá»‘c, plugin ecosystem phong phÃº
- **LobeChat** (15k â­) - Modern, mobile-first design
- **AnythingLLM** (3k â­) - All-in-one RAG system  
- **LibreChat** (13k â­) - Enterprise multi-user
- **BigAGI** (2k â­) - Developer workflows

### ğŸ§  **AI Providers (Free Tier)**
1. **Cerebras** - 1 triá»‡u tokens/ngÃ y ğŸ†
2. **Cloudflare Workers AI** - 10K neurons/ngÃ y
3. **GitHub Models** - Multimodal há»— trá»£
4. **OpenRouter** - Äa dáº¡ng models
5. **Google AI Studio** - Gemini ultra long context
6. **Together AI** - Models má»›i nháº¥t
7. **DeepInfra** - Backup
8. **Replicate** - Specialized
9. **Ollama Local** - Offline fallback
10. **Puter.js** - Emergency backup

### âš¡ **Smart Routing System**
- **Load balancing** tá»± Ä‘á»™ng
- **Failover** thÃ´ng minh  
- **Caching** Redis cho performance
- **Rate limiting** per model
- **Health monitoring** real-time
- **Budget control** tá»± Ä‘á»™ng

## ğŸ› ï¸ YÃªu Cáº§u Há»‡ Thá»‘ng

### **Tá»‘i Thiá»ƒu**
- RAM: 4GB
- Storage: 20GB free space
- CPU: 2 cores
- OS: Ubuntu 20.04+, CentOS 8+, macOS 12+
- Docker & Docker Compose

### **Khuyáº¿n Nghá»‹**
- RAM: 8GB+
- Storage: 50GB+ SSD
- CPU: 4+ cores  
- Network: Stable internet connection

## ğŸš€ CÃ i Äáº·t Nhanh (15 phÃºt)

### **BÆ°á»›c 1: Clone & Setup**
```bash
# Clone repository (hoáº·c copy files Ä‘Ã£ táº¡o)
git clone https://github.com/your-repo/open-webui-stack.git
cd open-webui-stack

# Make scripts executable
chmod +x *.sh
```

### **BÆ°á»›c 2: CÃ i Äáº·t Docker (náº¿u chÆ°a cÃ³)**
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install -y docker.io docker-compose
sudo systemctl enable docker
sudo systemctl start docker

# macOS
brew install --cask docker
```

### **BÆ°á»›c 3: Environment Configuration**
```bash
# Copy template vÃ  cáº¥u hÃ¬nh
cp .env.template .env

# Edit file .env vá»›i API keys cá»§a báº¡n
nano .env
```

### **BÆ°á»›c 4: Quick Install**
```bash
# Cháº¡y cÃ i Ä‘áº·t tá»± Ä‘á»™ng
./quick-install.sh

# Hoáº·c manual vá»›i docker-compose
docker-compose up -d
```

### **BÆ°á»›c 5: Verify**
```bash
# Check services status
docker-compose ps

# Kiá»ƒm tra logs
docker-compose logs -f open-webui
```

## ğŸ”‘ Cáº¥u HÃ¬nh API Keys

### **1. Cerebras (Khuyáº¿n Nghá»‹ Nháº¥t)**
```bash
# ÄÄƒng kÃ½: https://cloud.cerebras.ai/
# Free: 1 triá»‡u tokens/ngÃ y
CEREBRAS_API_KEY=sk-your-key-here
```

### **2. Cloudflare Workers AI**
```bash
# ÄÄƒng kÃ½: https://dash.cloudflare.com/
# Free: 10K neurons/ngÃ y
CLOUDFLARE_API_KEY=your-token
CLOUDFLARE_ACCOUNT_ID=your-account-id
```

### **3. GitHub Models**
```bash
# ÄÄƒng kÃ½: https://github.com/marketplace/models
# Create Personal Access Token
GITHUB_TOKEN=ghp_your-token
```

### **4. OpenRouter**
```bash
# ÄÄƒng kÃ½: https://openrouter.ai/
# Free tier available
OPENROUTER_API_KEY=sk-or-your-key
```

### **5. Google AI Studio**
```bash
# ÄÄƒng kÃ½: https://aistudio.google.com/
# Free Gemini access
GOOGLE_API_KEY=your-key
```

### **6. Together AI**
```bash
# ÄÄƒng kÃ½: https://api.together.xyz/
# $5 free credits
TOGETHER_API_KEY=your-key
```

## ğŸ“Š CÃ¡c Port & URLs

| Service | Port | URL | MÃ´ Táº£ |
|---------|------|-----|-------|
| **Open WebUI** | 3000 | http://localhost:3000 | Main chat interface |
| **LobeChat** | 3001 | http://localhost:3001 | Modern UI |
| **AnythingLLM** | 3002 | http://localhost:3002 | RAG system |
| **LibreChat** | 3003 | http://localhost:3003 | Enterprise |
| **BigAGI** | 3004 | http://localhost:3004 | Developer tools |
| **LiteLLM API** | 4000 | http://localhost:4000 | API Gateway |
| **Nginx** | 80/443 | http://localhost | Reverse proxy |
| **Grafana** | 3030 | http://localhost:3030 | Monitoring |
| **Prometheus** | 9090 | http://localhost:9090 | Metrics |

## ğŸ›ï¸ Quáº£n LÃ½ Há»‡ Thá»‘ng

### **Commands Há»¯u Ãch**
```bash
# Stop/Start services
docker-compose down
docker-compose up -d

# View logs
docker-compose logs -f [service-name]

# Restart specific service
docker-compose restart open-webui

# Scale services
docker-compose up -d --scale open-webui=2

# Backup data
docker run --rm -v open-webui-data:/data -v $(pwd):/backup alpine tar czf /backup/open-webui-backup.tar.gz /data

# Update images
docker-compose pull && docker-compose up -d
```

### **Health Checks**
```bash
# Check all services
curl http://localhost:4000/health/liveliness

# Check specific service
curl http://localhost:3000/api/v1/health
```

## ğŸ“ˆ Monitoring & Analytics

### **Grafana Dashboards**
- **Access**: http://localhost:3030
- **Default**: admin / admin123!@#
- **Dashboards**: System metrics, API usage, error rates

### **Key Metrics**
- **Request volume** per model
- **Response time** latency
- **Success/error rates** by provider
- **Token usage** vs limits
- **Cache hit rates**

## ğŸ›¡ï¸ Báº£o Máº­t & Performance

### **Security Features**
- Rate limiting per model
- API key validation
- CORS protection  
- Session management
- Data encryption

### **Performance Optimization**
- Redis caching
- Connection pooling
- Load balancing
- Health checks
- Automatic failover

## ğŸ”§ Troubleshooting

### **Common Issues**

#### 1. **Services khÃ´ng start**
```bash
# Check Docker status
docker --version
docker-compose --version

# View logs
docker-compose logs [service-name]

# Restart Docker
sudo systemctl restart docker
```

#### 2. **Port conflicts**
```bash
# Check port usage
netstat -tlnp | grep :3000

# Kill process using port
sudo lsof -ti:3000 | xargs kill -9
```

#### 3. **API Keys khÃ´ng work**
```bash
# Test key directly
curl -H "Authorization: Bearer YOUR_KEY" \
     https://api.cerebras.ai/v1/models

# Check .env file
cat .env | grep -E "API_KEY|ID"
```

#### 4. **Low performance**
```bash
# Monitor resource usage
docker stats

# Check disk space
df -h

# Monitor network
iftop -i eth0
```

### **Log Locations**
```
- Docker logs: ~/.docker/containers/
- Service logs: docker-compose logs [service]
- Nginx logs: ./logs/nginx/
- Database logs: ./logs/postgres/
```

## ğŸ“š Advanced Configuration

### **Custom Model Groups**
```yaml
# Edit litellm-config.yaml
model_groups:
  fast_inference:
    models: ["llama-3.1-8b-cerebras", "gpt-4o-mini-github"]
    
  high_quality:
    models: ["llama-3.1-70b-cerebras", "claude-3.5-sonnet-github"]
    
  coding_specialized:
    models: ["codellama-replicate", "qwen-2.5-together"]
```

### **Rate Limiting**
```yaml
# Per model limits
rate_limits:
  "llama-3.1-70b-cerebras":
    rpm: 100  # requests per minute
    rph: 2000 # requests per hour
```

### **Custom Environment**
```bash
# Development mode
LOG_LEVEL=debug
DEBUG_MODE=true

# Production mode  
LOG_LEVEL=info
DEBUG_MODE=false
ENABLE_MONITORING=true
```

## ğŸ¯ Usage Tips

### **Best Practices**
1. **Thá»© tá»± Æ°u tiÃªn**: Cerebras â†’ Cloudflare â†’ GitHub â†’ OpenRouter
2. **Monitoring**: Check Grafana dashboard daily
3. **Backup**: Automatic daily backups enabled
4. **Updates**: Run monthly image updates

### **Cost Optimization**
- Use Cerebras for bulk requests (highest quota)
- Use local Ollama for development/testing
- Monitor token usage in Grafana
- Set budget alerts at 80% usage

### **Performance Tuning**
- Enable caching: improves response time by 40%
- Use load balancing: distributes load evenly
- Monitor health checks: automatic failover
- Regular log rotation: prevents disk full

## ğŸ“ Support & Community

### **Resources**
- **Documentation**: Comprehensive guides included
- **API Reference**: http://localhost:4000/docs
- **Health Dashboard**: http://localhost:4000/health
- **Metrics**: http://localhost:9090 (Prometheus)

### **Getting Help**
1. Check logs: `docker-compose logs [service]`
2. Verify configuration: Review .env file
3. Test connectivity: Use provided curl commands
4. Monitor metrics: Check Grafana dashboards

---

## âœ… Káº¿t Quáº£ Mong Äá»£i

Sau khi cÃ i Ä‘áº·t thÃ nh cÃ´ng, báº¡n sáº½ cÃ³:

âœ… **5 giao diá»‡n AI** hoáº¡t Ä‘á»™ng Ä‘á»“ng thá»i  
âœ… **10+ nguá»“n API miá»…n phÃ­** Ä‘Æ°á»£c tÃ­ch há»£p  
âœ… **Smart routing** tá»± Ä‘á»™ng chá»n model tá»‘t nháº¥t  
âœ… **Load balancing** phÃ¢n phá»‘i táº£i hiá»‡u quáº£  
âœ… **Monitoring real-time** vá»›i Grafana  
âœ… **Failover tá»± Ä‘á»™ng** khi provider gáº·p sá»± cá»‘  
âœ… **Caching system** tÄƒng tá»‘c Ä‘á»™ pháº£n há»“i  
âœ… **Security layers** báº£o vá»‡ há»‡ thá»‘ng  

**Total: 5 UIs + 10 APIs = 15 cÃ¡ch truy cáº­p AI khÃ¡c nhau!**

ğŸ‰ **ChÃºc báº¡n sá»­ dá»¥ng hiá»‡u quáº£ há»‡ thá»‘ng AI máº¡nh máº½ nÃ y!**