# üéØ AI Model Auto-Filter & Management Guide

## V·∫•n ƒë·ªÅ b·∫°n g·∫∑p ph·∫£i
Khi c√≥ qu√° nhi·ªÅu provider v√† model, s·∫Ω c√≥:
- ‚ùå Nhi·ªÅu model kh√¥ng ho·∫°t ƒë·ªông (c·∫ßn API key)
- ‚ùå Model ch·∫≠m ho·∫∑c kh√¥ng ph·∫£n h·ªìi
- ‚ùå Kh√≥ t√¨m model ph√π h·ª£p cho t·ª´ng task
- ‚ùå L√£ng ph√≠ th·ªùi gian test t·ª´ng model

## Gi·∫£i ph√°p t·ª± ƒë·ªông

### 1. üìã L·ªçc Model T·ª± ƒê·ªông

```bash
# Ch·∫°y script l·ªçc model
chmod +x model-filter.sh
./model-filter.sh --full
```

Script s·∫Ω t·ª± ƒë·ªông:
- ‚úÖ Test t·∫•t c·∫£ model c√≥ s·∫µn
- ‚úÖ Ki·ªÉm tra API keys
- ‚úÖ T·∫°o danh s√°ch model working
- ‚úÖ T·∫°o c·∫•u h√¨nh l·ªçc cho t·ª´ng UI

### 2. üîß C·∫•u h√¨nh Model cho T·ª´ng UI

#### A. Open WebUI - Ch·ªâ hi·ªÉn th·ªã Model Working

**T·∫°o file c·∫•u h√¨nh ri√™ng:**
```javascript
// File: openwebui_filter_config.js
const FILTERED_MODELS = {
    // Local models (kh√¥ng c·∫ßn API key)
    "local_working": [
        {
            id: "llama3.1",
            name: "Llama 3.1 (Fast)",
            description: "Model nhanh nh·∫•t, local",
            provider: "ollama",
            category: "General Chat"
        },
        {
            id: "mistral:7b", 
            name: "Mistral 7B (Balanced)",
            description: "C√¢n b·∫±ng t·ªëc ƒë·ªô v√† ch·∫•t l∆∞·ª£ng",
            provider: "ollama",
            category: "General Chat"
        },
        {
            id: "codellama",
            name: "Code Llama (Programming)",
            description: "Chuy√™n code v√† programming",
            provider: "ollama", 
            category: "Programming"
        }
    ],
    
    // Cloud models (c·∫ßn API key nh∆∞ng ho·∫°t ƒë·ªông t·ªët)
    "cloud_working": [
        {
            id: "gpt-4o-mini",
            name: "GPT-4o Mini (Fast & Cheap)",
            description: "R·∫ª v√† nhanh nh·∫•t t·ª´ OpenAI",
            provider: "openai",
            category: "General Chat"
        },
        {
            id: "claude-3.5-haiku", 
            name: "Claude 3.5 Haiku (Quick)",
            description: "Nhanh v√† th√¥ng minh",
            provider: "anthropic",
            category: "General Chat"
        }
    ]
};

// Export for UI use
if (typeof module !== 'undefined') {
    module.exports = FILTERED_MODELS;
}
```

#### B. T·∫°o Model Filter Component cho UI

**Open WebUI Filter Component:**
```javascript
// File: model-filter-component.js
class ModelFilter {
    constructor(containerId) {
        this.container = document.getElementById(containerId);
        this.filteredModels = FILTERED_MODELS;
        this.currentFilter = 'all';
    }

    init() {
        this.createFilterUI();
        this.loadWorkingModels();
    }

    createFilterUI() {
        const filterHTML = `
            <div class="model-filter-container">
                <div class="filter-tabs">
                    <button class="filter-tab active" data-filter="all">All Models</button>
                    <button class="filter-tab" data-filter="local_working">Local (Free)</button>
                    <button class="filter-tab" data-filter="cloud_working">Cloud (API)</button>
                </div>
                <div class="model-status-indicator">
                    <span class="status-badge working">‚úÖ Working</span>
                    <span class="status-badge api-required">üîë API Required</span>
                </div>
            </div>
        `;
        this.container.innerHTML = filterHTML;
        this.bindEvents();
    }

    bindEvents() {
        const tabs = this.container.querySelectorAll('.filter-tab');
        tabs.forEach(tab => {
            tab.addEventListener('click', (e) => {
                this.switchFilter(e.target.dataset.filter);
            });
        });
    }

    loadWorkingModels() {
        const modelsContainer = document.getElementById('models-container');
        modelsContainer.innerHTML = '';
        
        let models = [];
        switch(this.currentFilter) {
            case 'local_working':
                models = this.filteredModels.local_working;
                break;
            case 'cloud_working':
                models = this.filteredModels.cloud_working;
                break;
            default:
                models = [...this.filteredModels.local_working, ...this.filteredModels.cloud_working];
        }
        
        this.renderModels(models);
    }

    renderModels(models) {
        const modelsContainer = document.getElementById('models-container');
        
        models.forEach(model => {
            const modelCard = this.createModelCard(model);
            modelsContainer.appendChild(modelCard);
        });
    }

    createModelCard(model) {
        const card = document.createElement('div');
        card.className = 'model-card';
        card.innerHTML = `
            <div class="model-header">
                <h3>${model.name}</h3>
                <span class="model-category">${model.category}</span>
            </div>
            <p class="model-description">${model.description}</p>
            <div class="model-footer">
                <span class="model-provider">${model.provider}</span>
                <button class="use-model-btn" data-model="${model.id}">Use Model</button>
            </div>
        `;
        return card;
    }

    switchFilter(filter) {
        this.currentFilter = filter;
        this.loadWorkingModels();
        
        // Update active tab
        this.container.querySelectorAll('.filter-tab').forEach(tab => {
            tab.classList.remove('active');
        });
        this.container.querySelector(`[data-filter="${filter}"]`).classList.add('active');
    }
}

// Initialize when DOM is loaded
document.addEventListener('DOMContentLoaded', () => {
    const filter = new ModelFilter('model-filter');
    filter.init();
});
```

### 3. üöÄ T·ª± ƒê·ªông Setup v√† Test

**Script t·ª± ƒë·ªông setup:**
```bash
#!/bin/bash
# File: auto-setup-models.sh

echo "ü§ñ Auto Setup Working Models"
echo "=========================="

# 1. Setup Ollama (local models)
setup_ollama() {
    echo "üì¶ Setting up Ollama..."
    curl -fsSL https://ollama.ai/install.sh | sh
    
    # Install working models
    echo "üì• Installing recommended models..."
    ollama pull llama3.1:8b
    ollama pull mistral:7b  
    ollama pull codellama:7b
    ollama pull phi3:mini
    
    echo "‚úÖ Ollama setup complete!"
}

# 2. Check and test API keys
check_api_keys() {
    echo "üîë Checking API keys..."
    
    # Load .env file
    source .env
    
    # Test OpenAI
    if [ -n "$OPENAI_API_KEY" ]; then
        echo "Testing OpenAI API..."
        curl -s -H "Authorization: Bearer $OPENAI_API_KEY" \
             https://api.openai.com/v1/models | grep -q "gpt-4o" && \
        echo "‚úÖ OpenAI API working" || echo "‚ùå OpenAI API failed"
    fi
    
    # Test Anthropic
    if [ -n "$ANTHROPIC_API_KEY" ]; then
        echo "Testing Anthropic API..."
        curl -s -H "x-api-key: $ANTHROPIC_API_KEY" \
             https://api.anthropic.com/v1/messages \
             -d '{"max_tokens": 5}' && \
        echo "‚úÖ Anthropic API working" || echo "‚ùå Anthropic API failed"
    fi
}

# 3. Create filtered configs
create_configs() {
    echo "‚öôÔ∏è Creating filtered configurations..."
    
    # Run model filter
    chmod +x model-filter.sh
    ./model-filter.sh --full
    
    # Update docker-compose with filtered models
    # (We'll add this in the next step)
}

# 4. Apply configurations
apply_configs() {
    echo "üîÑ Applying configurations..."
    
    # Update Open WebUI config
    if [ -f "openwebui_filter_config.js" ]; then
        cp openwebui_filter_config.js ./openwebui/data/
    fi
    
    # Update LiteLLM config  
    if [ -f "litellm_filtered_config.yaml" ]; then
        cp litellm_filtered_config.yaml ./litellm/config.yaml
    fi
    
    echo "‚úÖ Configurations applied!"
}

# Run all steps
main() {
    setup_ollama
    check_api_keys
    create_configs
    apply_configs
    
    echo ""
    echo "üéâ Auto setup complete!"
    echo ""
    echo "üìã Summary:"
    echo "- Local models: Ollama is running with filtered models"
    echo "- Cloud models: API keys tested and configured"
    echo "- UI configs: Filtered model lists created"
    echo ""
    echo "üöÄ Next steps:"
    echo "1. Start services: docker-compose up -d"
    echo "2. Open http://localhost:3000 for Open WebUI"
    echo "3. Models are pre-filtered and ready to use!"
}

main "$@"
```

### 4. üé® Enhanced UI Filters

**CSS cho Model Filter:**
```css
/* File: model-filter.css */
.model-filter-container {
    margin: 20px 0;
    padding: 20px;
    background: #f8f9fa;
    border-radius: 8px;
}

.filter-tabs {
    display: flex;
    gap: 10px;
    margin-bottom: 15px;
}

.filter-tab {
    padding: 8px 16px;
    border: none;
    background: #e9ecef;
    border-radius: 4px;
    cursor: pointer;
    transition: all 0.3s;
}

.filter-tab.active {
    background: #007bff;
    color: white;
}

.filter-tab:hover {
    background: #0056b3;
    color: white;
}

.model-status-indicator {
    display: flex;
    gap: 15px;
    margin-bottom: 15px;
}

.status-badge {
    padding: 4px 8px;
    border-radius: 4px;
    font-size: 12px;
    font-weight: bold;
}

.status-badge.working {
    background: #d4edda;
    color: #155724;
}

.status-badge.api-required {
    background: #fff3cd;
    color: #856404;
}

.model-card {
    background: white;
    border: 1px solid #dee2e6;
    border-radius: 8px;
    padding: 15px;
    margin: 10px 0;
    transition: all 0.3s;
}

.model-card:hover {
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    transform: translateY(-2px);
}

.model-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 10px;
}

.model-category {
    background: #e9ecef;
    padding: 2px 8px;
    border-radius: 12px;
    font-size: 12px;
}

.use-model-btn {
    background: #007bff;
    color: white;
    border: none;
    padding: 8px 16px;
    border-radius: 4px;
    cursor: pointer;
}

.use-model-btn:hover {
    background: #0056b3;
}
```

### 5. üìä Model Performance Dashboard

**Real-time Model Status:**
```javascript
// File: model-status-monitor.js
class ModelStatusMonitor {
    constructor() {
        this.models = new Map();
        this.updateInterval = 30000; // 30 seconds
    }

    async checkModelStatus(modelId, provider) {
        try {
            const start = Date.now();
            let response;
            
            switch(provider) {
                case 'ollama':
                    response = await this.testOllamaModel(modelId);
                    break;
                case 'openai':
                    response = await this.testOpenAIModel(modelId);
                    break;
                case 'anthropic':
                    response = await this.testAnthropicModel(modelId);
                    break;
                default:
                    response = { status: 'unknown', latency: 0 };
            }
            
            const latency = Date.now() - start;
            
            this.models.set(modelId, {
                status: response.status,
                latency: latency,
                lastChecked: new Date(),
                provider: provider
            });
            
            return { status: response.status, latency: latency };
        } catch (error) {
            this.models.set(modelId, {
                status: 'error',
                latency: 0,
                lastChecked: new Date(),
                provider: provider,
                error: error.message
            });
            return { status: 'error', error: error.message };
        }
    }

    async testOllamaModel(modelId) {
        const response = await fetch('http://localhost:11434/api/generate', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model: modelId,
                prompt: 'Hello',
                stream: false
            })
        });
        
        if (response.ok) {
            return { status: 'online' };
        } else {
            return { status: 'offline' };
        }
    }

    async testOpenAIModel(modelId) {
        // Requires API key, would need to be implemented with actual key
        return { status: 'unknown' };
    }

    getWorkingModels() {
        const working = [];
        this.models.forEach((status, modelId) => {
            if (status.status === 'online') {
                working.push({ id: modelId, ...status });
            }
        });
        return working;
    }

    getSlowModels(threshold = 3000) {
        const slow = [];
        this.models.forEach((status, modelId) => {
            if (status.status === 'online' && status.latency > threshold) {
                slow.push({ id: modelId, ...status });
            }
        });
        return slow;
    }

    startMonitoring() {
        setInterval(() => {
            this.checkAllModels();
        }, this.updateInterval);
    }

    async checkAllModels() {
        const modelList = this.getModelList();
        for (const model of modelList) {
            await this.checkModelStatus(model.id, model.provider);
        }
    }

    getModelList() {
        // This would be loaded from your filtered models config
        return [
            { id: 'llama3.1', provider: 'ollama' },
            { id: 'mistral:7b', provider: 'ollama' },
            { id: 'codellama', provider: 'ollama' },
            { id: 'gpt-4o-mini', provider: 'openai' },
            { id: 'claude-3.5-haiku', provider: 'anthropic' }
        ];
    }
}

// Initialize monitor
document.addEventListener('DOMContentLoaded', () => {
    const monitor = new ModelStatusMonitor();
    monitor.startMonitoring();
    
    // Update UI with status
    setInterval(() => {
        updateModelStatusUI(monitor);
    }, 1000);
});

function updateModelStatusUI(monitor) {
    const workingModels = monitor.getWorkingModels();
    const slowModels = monitor.getSlowModels();
    
    console.log(`Working models: ${workingModels.length}`);
    console.log(`Slow models: ${slowModels.length}`);
    
    // Update UI elements with status
    // This would integrate with your actual UI
}
```

## üéØ K·∫øt Qu·∫£ Mong ƒê·ª£i

Sau khi √°p d·ª•ng gi·∫£i ph√°p n√†y:

1. **‚úÖ Ch·ªâ th·∫•y model ho·∫°t ƒë·ªông**: Kh√¥ng c√≤n th·∫•y model l·ªói ho·∫∑c c·∫ßn API key
2. **‚ö° T·ª± ƒë·ªông test v√† l·ªçc**: Script t·ª± ƒë·ªông ki·ªÉm tra v√† c·∫≠p nh·∫≠t danh s√°ch
3. **üè∑Ô∏è Ph√¢n lo·∫°i r√µ r√†ng**: Local (mi·ªÖn ph√≠) vs Cloud (API) vs Programming
4. **üìä Theo d√µi hi·ªáu su·∫•t**: ƒêo t·ªëc ƒë·ªô v√† ƒë·ªô ·ªïn ƒë·ªãnh c·ªßa t·ª´ng model
5. **üîß C·∫•u h√¨nh d·ªÖ d√†ng**: Ch·ªâ c·∫ßn ch·∫°y 1 script ƒë·ªÉ setup everything

## üöÄ B·∫Øt ƒê·∫ßu Ngay

```bash
# Ch·∫°y l·ªçc model ngay l·∫≠p t·ª©c
chmod +x model-filter.sh
./model-filter.sh --full

# Ho·∫∑c ch·∫°y auto setup
chmod +x auto-setup-models.sh  
./auto-setup-models.sh
```

V·ªõi gi·∫£i ph√°p n√†y, b·∫°n s·∫Ω kh√¥ng c√≤n ph·∫£i ƒë·ªëi m·∫∑t v·ªõi danh s√°ch model r·ªëi r·∫Øm n·ªØa! üéâ